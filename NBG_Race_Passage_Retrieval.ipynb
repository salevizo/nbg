{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NBG-Race-Passage-Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salevizo/nbg/blob/master/NBG_Race_Passage_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POcO9bxv39wF",
        "colab_type": "text"
      },
      "source": [
        "# NBG Race Passage Retrieval Baseline Model\n",
        "This Colab Notebook is an easy way to run our baseline model in order to get more confident on how and where to start from. Our model serves as a reference on handling the inputs and outputs, a quick way to get you started without losing too much time on data manipulation issues. A big part of this piece of code can be used unchanged in the solution that you will submit.\n",
        "\n",
        "## Philosophy behind the Baseline Model\n",
        "We have created a Supervised approach on this Passage Retrieval task which achieves decent results by applying a heuristic technique. The core idea behind our approach is that legal texts/court decisions have a strict writing format and one can expect many similarities among the structure of different documents which share the same topic. This way if someone had the answers to some questions asked against some documents, he/she could use them to learn a pattern, find a general structure and some keywords that acompany this answer.\n",
        "\n",
        "Our model uses some known answers (Answers200.json) to questions (questions.csv) and documents (Documents200.json) to create its own augmented question that will replace the original one. This augmented question consists of all the unique words found in answers from all the documents we fed to train with.\n",
        "\n",
        "The model will iterate over all the documents in the test dataframe and tokenize each document to sentences. Then for every question in the questions.csv file it will calculate its vector and it will start iterating through the trained questions in order to find the best match/the most similar one to the original (by calculating the cosine similarity between them). \n",
        "\n",
        "> *In the baseline model that we provide you with, this step isn't really necessary because all questions take part in the training. We have this mechanism of finding the most similar question from the training ones because in the future we will test the models you will submit against paraphrased questions. Keep that in mind when you develop your solution.*\n",
        "\n",
        "When the model finally has created a question vector it will calculate the cosine similarity between that and the vector for every sentence in the document. Those results will be saved in a list of dictionaries which we will use to get the top ranked/most similar sentences. Finally, we will take the top 4 most similar sentences, to create a passage that will serve as the estimate passage of our model. Those passages are being saved along with the document id and the question id in a dataframe that at the end of the execution will be our estimated_answers.csv output file. This file along with the Answers200.json will be feeded to the **calculate_answers_score** function of the **answers_evaluation** python script that you can find in the github project. The console will print the Average F1 Score for all the question, the F1 Score per question, the Critical Success Index and the Overall Score.\n",
        "\n",
        "\n",
        "### Some of the model's limitations\n",
        "*   This model won't consider if a question has an answer or not. It will answer all the questions, resulting to a worse CSI score.\n",
        "*   This model doesn't have any 'smart' mechanism on which sentences form a passage, it just takes the n most similar sentences found in the document to the question given.\n",
        "\n",
        "## Conclusion\n",
        "Feel free to play arround and tweak parameters to see immediate performance changes, train your own vectors to use.\n",
        "We do not want in any way to restrict you regarding the approach you will take, by giving this model. Think out of the box and show us different ways to achieve better results with respect to the desired output.\n",
        "\n",
        "Find the code in the Github repo: https://github.com/myNBGcode/NBG-Race-passage-retrieval.git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ImFkbCmrrie",
        "colab_type": "code",
        "outputId": "6b2bcde6-fd4a-432a-af9c-f6701f3439d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Clone NBG-Race-passage-retrieval Github project\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "AUTHENTICATION = '{}:{}'.format(user, password)\n",
        "\n",
        "!git clone https://$AUTHENTICATION@github.com/myNBGcode/NBG-Race-passage-retrieval.git\n",
        "AUTHENTICATION = '' # removing authentication variable\n",
        "\n",
        "#Get into project's directory and view its content\n",
        "%cd NBG-Race-passage-retrieval\n",
        "%ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: ZisisFl\n",
            "Password: ··········\n",
            "Cloning into 'NBG-Race-passage-retrieval'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Total 19 (delta 0), reused 0 (delta 0), pack-reused 19\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n",
            "Checking out files: 100% (14/14), done.\n",
            "/content/NBG-Race-passage-retrieval\n",
            "answers_evaluation.py  passage_retrieval_supervised.py  ROUGE_N_algorithm.py\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BuM59-k2XxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "from answers_evaluation import calculate_answers_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywwS89tEHLCk",
        "colab_type": "text"
      },
      "source": [
        "### text_processing\n",
        "This function is used to process text and get it ready for word vectors calculation/the same function was used as a preprocessing step for the creation of the words \n",
        "vectors we have provided you with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBg8p2EO20XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def text_processing(text):\n",
        "    lower_case_text = text.lower()\n",
        "    # remove non words\n",
        "    clean = re.sub(\"[^Α-ΩΆΈΌΊΏΉΎα-ωάέόίώήύϊϋ]\", \" \", lower_case_text)\n",
        "    split_to_tokens = clean.split()\n",
        "    split_to_tokens = [w for w in split_to_tokens if w not in stop_words]\n",
        "    return split_to_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voEfDm6a22eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_mean_vector(word2vec_model, words):\n",
        "    # remove out-of-vocabulary words\n",
        "    words = [word for word in words if word in word2vec_model.vocab]\n",
        "    if len(words) >= 1:\n",
        "        return np.mean(word2vec_model[words], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NKs4Jw7239C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "    return np.dot(vector1, vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HAyY0ww25xH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_unique_words(text):\n",
        "    unique_words = []\n",
        "    processed_text = text_processing(text)\n",
        "    for token in processed_text:\n",
        "        if not token in unique_words:\n",
        "            unique_words.append(token)\n",
        "\n",
        "    return unique_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfOEP1EOHc_i",
        "colab_type": "text"
      },
      "source": [
        "### create_unique_keywords_list\n",
        "This funtion is used for the augmentation of the questions based on answers given. This function can serve as a reference on how to manipulate the complex json file formats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8WeoqUu29jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_unique_keywords_list(input_df):\n",
        "    # init dataframe that will hold the concatenated corpus for each question\n",
        "    concatenated_answers = pd.DataFrame(columns=['concatenated_answer'])\n",
        "\n",
        "    for index, row in input_df.iterrows():\n",
        "        # create column with ekoiphsh class as a simple text\n",
        "        df.loc[index, 'annotation.classificationResult'] = row['annotation.classificationResult'][0]['classes'][0]\n",
        "\n",
        "        # generate a dataframe of concatenated answers on every different question found in the input_df\n",
        "        normalized_df = json_normalize(row['annotation.annotationResult'])\n",
        "        for index, row in normalized_df.iterrows():\n",
        "            for i in range(len(row['label'])):\n",
        "                # if this question category exists in the dataframe overwrite text with old + new\n",
        "                if row['label'][i] in concatenated_answers.index:\n",
        "                    concatenated_answers.loc[row['label'][i]] = row['points'][0]['text'] + ' ' + \\\n",
        "                                                                concatenated_answers.loc[row['label'][i]]\n",
        "                # if this question category doesn't exist in the dataframe insert it\n",
        "                else:\n",
        "                    concatenated_answers.loc[row['label'][i]] = row['points'][0]['text']\n",
        "    concatenated_answers['question_id'] = concatenated_answers.index\n",
        "\n",
        "    # generate list of unique keywords from concatenated answers in order to be used as trained replacement questions\n",
        "    list_of_question_keywords = []\n",
        "    for index, row in concatenated_answers.iterrows():\n",
        "        list_of_question_keywords.append(get_unique_words(row['concatenated_answer']))\n",
        "    concatenated_answers['keywords_concat_answers'] = list_of_question_keywords\n",
        "\n",
        "    return concatenated_answers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9a2pNUj2-ed",
        "colab_type": "code",
        "outputId": "5d5c24e3-7a0d-47ea-80b0-664319956966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# load nltk greek tokenizer\n",
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/greek.pickle')\n",
        "\n",
        "\n",
        "# load trained vectors\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join('data', 'vectors', 'FEK_ABDG_100.bin'), binary=True)\n",
        "\n",
        "\n",
        "# load greek stop words\n",
        "stop_words = []\n",
        "with open(os.path.join('data', 'stop_words.txt'), 'r', encoding='utf-8') as stop_words_file:\n",
        "    for line in stop_words_file.readlines():\n",
        "        stop_words.append(line[:-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b9d5aNv3BOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dataframe of  questions\n",
        "processed_questions = []\n",
        "questions_df = pd.read_csv(os.path.join('data', 'Questions.csv'))\n",
        "for index, row in questions_df.iterrows():\n",
        "    # process the query in order to be ready for vector calculation\n",
        "    query = text_processing(row['original_question'])\n",
        "    processed_questions.append(query)\n",
        "# append a new column to the questions dataframe that contains the queries processed\n",
        "questions_df['processed_question'] = processed_questions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CPqFVAv3F4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init answers dataframe, in this file the model will write the estimated answers\n",
        "answers_df = pd.DataFrame(columns=['document_id', 'question_id', 'estimated_passage'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti9i2CXTDWU5",
        "colab_type": "text"
      },
      "source": [
        "### Import Answers200.json and Documents200.json files\n",
        "#### Answers200.json\n",
        "It's time to import the file that contains the annotated answers that we are going to use in order to augment our queries. These answers will be carried by the train variable.\n",
        "\n",
        "#### Documents200.json\n",
        "We also import the documents that we will use to estimate the answer passages from. We exclude the documents that were used for training, so the rest of the documents will consist the test.\n",
        "\n",
        "####Important\n",
        "Answer200.json and Document200.json concern the same documents. This is why we drop the fraction of dataframe used for training from the test dataframe. When  you will receive the Document1000.json file you won't have to split in train and test because Document1000.json in its entirity will be the test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaLXG2xV3I7h",
        "colab_type": "code",
        "outputId": "5e3b701a-f914-4082-81de-2e8c16b53d76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# read the json file with the annotated answers\n",
        "with open(os.path.join('data', 'Answers200.json'), 'r', encoding='utf-8') as input_json:\n",
        "    json_file = json.loads(input_json.read())\n",
        "\n",
        "# normalize json file to dataframe\n",
        "df = json_normalize(json_file['data'])\n",
        "\n",
        "# use a random sample of the answers to create a train dataframe\n",
        "train = df.sample(frac=0.25)\n",
        "\n",
        "# read the json file that contains the documents\n",
        "with open(os.path.join('data', 'Documents200.json'), 'r', encoding='utf-8') as input_json:\n",
        "    json_file = json.loads(input_json.read())\n",
        "\n",
        "test = json_normalize(json_file['data'])\n",
        "# remove the documents that where used for training (this won't be necessary when you receive the Documents1000.json)\n",
        "test = test.drop(train.index)\n",
        "\n",
        "''' \n",
        "    CODE TO REPLACE FOR THE Documents1000.json file\n",
        "    with open(os.path.join('data', 'Documents1000.json'), 'r', encoding='utf-8') as input_json:\n",
        "        json_file = json.loads(input_json.read())\n",
        "        \n",
        "    test = json_normalize(json_file['data'])\n",
        "    # do not drop from test\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n    CODE TO REPLACE FOR THE Documents1000.json file\\n    with open(os.path.join('data', 'Documents1000.json'), 'r', encoding='utf-8') as input_json:\\n        json_file = json.loads(input_json.read())\\n        \\n    test = json_normalize(json_file['data'])\\n    # do not drop from test\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upt-7ZlSFbSe",
        "colab_type": "text"
      },
      "source": [
        "### Question augmentation/training happens here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6O0jPO93Pkl",
        "colab_type": "code",
        "outputId": "ac57090e-0032-437d-a860-44169c61f539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# augment questions from the known given answers\n",
        "# we create a dataframe to replace every question included in the train dataframe with lists of unique words\n",
        "# found in the answers of those questions\n",
        "trained_questions_df = create_unique_keywords_list(train)\n",
        "\n",
        "# merge the two dataframes to bring original question and processed question data to trained_questions_df\n",
        "trained_questions_df = trained_questions_df.merge(questions_df, left_on='question_id', right_on='question_id')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4TYth_xFE6k",
        "colab_type": "text"
      },
      "source": [
        "### Here starts the actual model\n",
        "This is the code block that you will have to apply the most changes to get different results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_VGfwDL3SuN",
        "colab_type": "code",
        "outputId": "c634bd86-4ce7-48ed-e20d-3b67bb97339c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# init a counter\n",
        "document_count = 0\n",
        "\n",
        "# loop through documents\n",
        "for index, document in test.iterrows():\n",
        "    # tokenize to sentences the document\n",
        "    raw_sentences = tokenizer.tokenize(document['content'])\n",
        "\n",
        "    document_count = document_count + 1\n",
        "    print('Processing document {} out of {}'.format(str(document_count), str(len(test.index))))\n",
        "\n",
        "    # for every question\n",
        "    for q_index, question in questions_df.iterrows():\n",
        "        # if there is a trained question version of an original question, use that instead this applies\n",
        "        # this method will apply for questions that are rephrased so we can still levarage the trained ones\n",
        "\n",
        "        # init question vector with the original question\n",
        "        question_vector = calc_mean_vector(model, question['processed_question'])\n",
        "        similar_question_vector = ''\n",
        "\n",
        "        best_sim = 0\n",
        "        best_candidate = None\n",
        "        # for every question found in the trained ones\n",
        "        for index2, proc_question in trained_questions_df.iterrows():\n",
        "            # calculate the candidate similar question vector\n",
        "            similar_question_vector = calc_mean_vector(model, proc_question['processed_question'])\n",
        "\n",
        "            # calculate the cosine similarity of the two vectors\n",
        "            similarity = cosine_similarity(question_vector, similar_question_vector)\n",
        "\n",
        "            if similarity > best_sim:\n",
        "                best_sim = similarity\n",
        "                best_candidate = proc_question['keywords_concat_answers']\n",
        "\n",
        "        # calculate the question vector using the best candidate / we assume here that a big enough training\n",
        "        # set is used and all the question take part in the training\n",
        "        question_vector = calc_mean_vector(model, best_candidate)\n",
        "\n",
        "        # init list of results for the similarity of each doc's sentence with the question\n",
        "        list_of_results = []\n",
        "        # for every question in the document\n",
        "        for i in range(len(raw_sentences)):\n",
        "            # tokenize and process sentence\n",
        "            tokenized_sentence = text_processing(raw_sentences[i])\n",
        "            # calculate the mean vector of the sentence's tokens\n",
        "            sentence_vector = calc_mean_vector(model, tokenized_sentence)\n",
        "\n",
        "            # if a sentence's tokens exist in the vocabulary calculate cosine similarity\n",
        "            if sentence_vector is not None:\n",
        "                similarity = cosine_similarity(question_vector, sentence_vector)\n",
        "                list_of_results.append(\n",
        "                    {'sentence_index': i, 'original_sentence': raw_sentences[i], 'similarity': similarity})\n",
        "\n",
        "        # sort by similarity the list of the results\n",
        "        list_of_results = sorted(list_of_results, key=lambda k: k['similarity'], reverse=True)\n",
        "\n",
        "        # create a passage from the top n ranked by similarity sentences\n",
        "        estimated_passage = ''\n",
        "        top_n = 4\n",
        "        for i in range(top_n):\n",
        "            estimated_passage = estimated_passage + raw_sentences[list_of_results[i]['sentence_index']] + ' '\n",
        "\n",
        "        answers_df = answers_df.append(\n",
        "            {'document_id': document['id'], 'question_id': question['question_id'], 'estimated_passage': estimated_passage},\n",
        "            ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing document 1 out of 150\n",
            "Processing document 2 out of 150\n",
            "Processing document 3 out of 150\n",
            "Processing document 4 out of 150\n",
            "Processing document 5 out of 150\n",
            "Processing document 6 out of 150\n",
            "Processing document 7 out of 150\n",
            "Processing document 8 out of 150\n",
            "Processing document 9 out of 150\n",
            "Processing document 10 out of 150\n",
            "Processing document 11 out of 150\n",
            "Processing document 12 out of 150\n",
            "Processing document 13 out of 150\n",
            "Processing document 14 out of 150\n",
            "Processing document 15 out of 150\n",
            "Processing document 16 out of 150\n",
            "Processing document 17 out of 150\n",
            "Processing document 18 out of 150\n",
            "Processing document 19 out of 150\n",
            "Processing document 20 out of 150\n",
            "Processing document 21 out of 150\n",
            "Processing document 22 out of 150\n",
            "Processing document 23 out of 150\n",
            "Processing document 24 out of 150\n",
            "Processing document 25 out of 150\n",
            "Processing document 26 out of 150\n",
            "Processing document 27 out of 150\n",
            "Processing document 28 out of 150\n",
            "Processing document 29 out of 150\n",
            "Processing document 30 out of 150\n",
            "Processing document 31 out of 150\n",
            "Processing document 32 out of 150\n",
            "Processing document 33 out of 150\n",
            "Processing document 34 out of 150\n",
            "Processing document 35 out of 150\n",
            "Processing document 36 out of 150\n",
            "Processing document 37 out of 150\n",
            "Processing document 38 out of 150\n",
            "Processing document 39 out of 150\n",
            "Processing document 40 out of 150\n",
            "Processing document 41 out of 150\n",
            "Processing document 42 out of 150\n",
            "Processing document 43 out of 150\n",
            "Processing document 44 out of 150\n",
            "Processing document 45 out of 150\n",
            "Processing document 46 out of 150\n",
            "Processing document 47 out of 150\n",
            "Processing document 48 out of 150\n",
            "Processing document 49 out of 150\n",
            "Processing document 50 out of 150\n",
            "Processing document 51 out of 150\n",
            "Processing document 52 out of 150\n",
            "Processing document 53 out of 150\n",
            "Processing document 54 out of 150\n",
            "Processing document 55 out of 150\n",
            "Processing document 56 out of 150\n",
            "Processing document 57 out of 150\n",
            "Processing document 58 out of 150\n",
            "Processing document 59 out of 150\n",
            "Processing document 60 out of 150\n",
            "Processing document 61 out of 150\n",
            "Processing document 62 out of 150\n",
            "Processing document 63 out of 150\n",
            "Processing document 64 out of 150\n",
            "Processing document 65 out of 150\n",
            "Processing document 66 out of 150\n",
            "Processing document 67 out of 150\n",
            "Processing document 68 out of 150\n",
            "Processing document 69 out of 150\n",
            "Processing document 70 out of 150\n",
            "Processing document 71 out of 150\n",
            "Processing document 72 out of 150\n",
            "Processing document 73 out of 150\n",
            "Processing document 74 out of 150\n",
            "Processing document 75 out of 150\n",
            "Processing document 76 out of 150\n",
            "Processing document 77 out of 150\n",
            "Processing document 78 out of 150\n",
            "Processing document 79 out of 150\n",
            "Processing document 80 out of 150\n",
            "Processing document 81 out of 150\n",
            "Processing document 82 out of 150\n",
            "Processing document 83 out of 150\n",
            "Processing document 84 out of 150\n",
            "Processing document 85 out of 150\n",
            "Processing document 86 out of 150\n",
            "Processing document 87 out of 150\n",
            "Processing document 88 out of 150\n",
            "Processing document 89 out of 150\n",
            "Processing document 90 out of 150\n",
            "Processing document 91 out of 150\n",
            "Processing document 92 out of 150\n",
            "Processing document 93 out of 150\n",
            "Processing document 94 out of 150\n",
            "Processing document 95 out of 150\n",
            "Processing document 96 out of 150\n",
            "Processing document 97 out of 150\n",
            "Processing document 98 out of 150\n",
            "Processing document 99 out of 150\n",
            "Processing document 100 out of 150\n",
            "Processing document 101 out of 150\n",
            "Processing document 102 out of 150\n",
            "Processing document 103 out of 150\n",
            "Processing document 104 out of 150\n",
            "Processing document 105 out of 150\n",
            "Processing document 106 out of 150\n",
            "Processing document 107 out of 150\n",
            "Processing document 108 out of 150\n",
            "Processing document 109 out of 150\n",
            "Processing document 110 out of 150\n",
            "Processing document 111 out of 150\n",
            "Processing document 112 out of 150\n",
            "Processing document 113 out of 150\n",
            "Processing document 114 out of 150\n",
            "Processing document 115 out of 150\n",
            "Processing document 116 out of 150\n",
            "Processing document 117 out of 150\n",
            "Processing document 118 out of 150\n",
            "Processing document 119 out of 150\n",
            "Processing document 120 out of 150\n",
            "Processing document 121 out of 150\n",
            "Processing document 122 out of 150\n",
            "Processing document 123 out of 150\n",
            "Processing document 124 out of 150\n",
            "Processing document 125 out of 150\n",
            "Processing document 126 out of 150\n",
            "Processing document 127 out of 150\n",
            "Processing document 128 out of 150\n",
            "Processing document 129 out of 150\n",
            "Processing document 130 out of 150\n",
            "Processing document 131 out of 150\n",
            "Processing document 132 out of 150\n",
            "Processing document 133 out of 150\n",
            "Processing document 134 out of 150\n",
            "Processing document 135 out of 150\n",
            "Processing document 136 out of 150\n",
            "Processing document 137 out of 150\n",
            "Processing document 138 out of 150\n",
            "Processing document 139 out of 150\n",
            "Processing document 140 out of 150\n",
            "Processing document 141 out of 150\n",
            "Processing document 142 out of 150\n",
            "Processing document 143 out of 150\n",
            "Processing document 144 out of 150\n",
            "Processing document 145 out of 150\n",
            "Processing document 146 out of 150\n",
            "Processing document 147 out of 150\n",
            "Processing document 148 out of 150\n",
            "Processing document 149 out of 150\n",
            "Processing document 150 out of 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBtVrLEE3XeB",
        "colab_type": "code",
        "outputId": "506af0ee-ab55-4d9b-f370-6917abf58100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# save answers in a CSV file\n",
        "answers_df.to_csv(os.path.join('data', 'estimated_answers.csv'), index=False, encoding='utf-8')\n",
        "print('\\nCSV file with answers saved in data directory')\n",
        "print('---------------------------------------------\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CSV file with answers saved in data directory\n",
            "---------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1seiIRTT3ZTi",
        "colab_type": "code",
        "outputId": "3f9076c3-d18e-4fda-c700-b68354c7078e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# calculate scores\n",
        "calculate_answers_score('estimated_answers.csv', 'Answers200.json')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NBG-Race-passage-retrieval/answers_evaluation.py:9: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  df = json_normalize(json_file['data'])\n",
            "/content/NBG-Race-passage-retrieval/answers_evaluation.py:13: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  normalized_df = json_normalize(row['annotation.annotationResult'])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average F1 Score (for questions with an answer): 0.27\n",
            "Critical Success Index: 0.89\n",
            "Overall Score: 0.40\n",
            "\n",
            "Average F1 Score per question category\n",
            "PISTWTRIA_TRAPEZA: 0.40\n",
            "PERIOYSIAKH_KATASTASH: 0.37\n",
            "OIKOGENEIAKH_KATATASTASH: 0.17\n",
            "RYTHMISI_OFEILWN: 0.18\n",
            "XARAKTHRISTIKA_EKPOIHSHS: 0.19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.27162506986324536, 0.8906666666666667, 0.3954333892239297)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqZUCtSdLf0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}